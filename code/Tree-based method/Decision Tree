#5.Dicision Tree
#5.1 Decision Tree via Gini index
dt_model<-rpart(formula=quality.code~., data=train_set, method="class", parms=list(split= "gini"),
  control=rpart.control(maxdepth=5, minsplit= 20, minbucket=10, cp= 0.001,maxsurrogate= 0))

#Select the optimal cp and prune according to the cross-validation results
best_cp <- dt_model$cptable[which.min(dt_model$cptable[,"xerror"]), "CP"]
dt_pruned <- prune(dt_model, cp = best_cp)

cp_tab <-as.data.frame(dt_model$cptable)
cp_tab$TreeSize<-cp_tab$nsplit+1

#Compute accuracy on training and test sets for each CP
train_acc<-sapply(cp_tab$CP, function(cp_val){
  pruned<- prune(dt_model, cp=cp_val)
  mean(predict(pruned, train_set, type="class")==train_set$quality.code)
})
test_acc<-sapply(cp_tab$CP, function(cp_val){
  pruned<-prune(dt_model, cp=cp_val)
  mean(predict(pruned, test_set, type="class")== test_set$quality.code)
})

#10-fold cv to select the parameter conbination
ctrl<-trainControl(method= "cv", number=10)
grid<-data.frame(cp=cp_tab$CP)

cv_fit <- train(quality.code ~ ., data=train_set, method="rpart", trControl=ctrl, 
                tuneGrid=grid,parms=list(split="gini"), 
                control=rpart.control(maxdepth=19, minspli=5,minbucket=3, cp=0.001,maxsurrogate= 0))
cv_results<-cv_fit$results
cv_acc<-cv_results$Accuracy[match(cp_tab$CP,cv_results$cp)]

df<-data.frame(TreeSize=cp_tab$TreeSize,Training=train_acc,
               CrossValidation=cv_acc,Test=test_acc)

df_long<-reshape2::melt(df, id.vars="TreeSize",
  variable.name="Dataset",value.name="Accuracy")

ggplot(df_long, aes(x=TreeSize, y=Accuracy, color=Dataset)) +
  geom_line(size=1) +geom_point(size=3) +
  scale_x_continuous(breaks=df$TreeSize) +scale_y_continuous(labels=percent_format(accuracy= 1)) +
  scale_color_manual(values=c(Training="#1f78b4", CrossValidation="#33a02c", Test="#ff7f00"))+
  labs(title= "Accuracy Comparison Across Tree Sizes",
      subtitle=sprintf("Optimal Pruning CP = %.5f (MaxDepth = %d, MinSplit = %d)",
      cp_tab$CP[which.max(cv_acc)], dt_model$control$maxdepth, dt_model$control$minsplit),
      x="Number of Leaf Nodes (Tree Size)", y="Accuracy", color =NULL)+
  theme_minimal(base_family ="Arial") +
  theme(
    plot.title= element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle=element_text(size = 11, color = "gray40", hjust = 0.5),
    axis.title=element_text(size = 12),
    axis.text=element_text(size = 10),
    legend.position=c(0.8, 0.2),
    legend.background=element_rect(fill = alpha("white", 0.6), color = NA),
    panel.grid.major=element_line(color = "gray90"),
    panel.grid.minor=element_blank())

#Visualize the tree structure before and after pruning
par(mar=c(1, 1, 2, 1))
rpart.plot(dt_model,main= "Original Tree",type=2,extra=104,fallen.leaves=TRUE,uniform=TRUE, 
           cex=NULL,tweak=1.3,split.cex=1.3,under.cex=1,xcompact=FALSE,ycompact=FALSE)
rpart.plot(dt_pruned, main="Tree after pruning", type=2, extra=104,
           fallen.leaves=TRUE, uniform=TRUE, cex=NULL,tweak=1.1,
           split.cex=1.3, under.cex=1, xcompact= FALSE, ycompact=FALSE)

#Predict on the test set
pred_class<-predict(dt_pruned,test_set, type ="class")
pred_prob<-predict(dt_pruned, test_set,type="prob")[,2] 

dt_roc <- roc(response=test_set$quality.code,predictor=pred_prob,
              levels=c("0","1"),direction = "<")
dt_auc<-auc(dt_roc)
roc_df<-data.frame(
  Model = "Decision Tree",
  FPR=1-dt_roc$specificities,
  TPR=dt_roc$sensitivities
)

#ROC curve
ggplot(roc_df, aes(x=FPR, y=TPR, color=Model)) +
  geom_line(size =0.7) +
  geom_abline(intercept=0, slope =1, linetype="dashed", color="gray") +
  labs(title="ROC Curve for Decision Tree", subtitle=paste("AUC =", round(dt_auc, 4)),
       x="False Positive Rate (FPR)",y="True Positive Rate (TPR)")+
  theme_minimal() +
  scale_color_manual(values = "#377EB8")+
  theme(legend.position="none",plot.title=element_text(face= "bold", size=16, hjust=0.5),
    plot.subtitle=element_text(size=12, hjust =0.5),axis.title=element_text(size=12))

cm_dt<-table(Predicted = pred_class, Actual = test_set$quality.code)
print(cm_dt)
tp<-cm_dt[2,2]
tn<-cm_dt[1,1]
fp<-cm_dt[2,1]
fn<-cm_dt[1,2]
metrics<-c(
  Accuracy=(tp+tn)/sum(cm_dt),
  Precision=tp/(tp+fp),
  Recall=tp/ (tp+ fn),
  Specificity= tn/(tn +fp),
  FPR=fp/(fp+tn),
  F1= 2*tp/(2*tp+fp+fn))
print(round(metrics, 4))

#Scatter plot
top2<-names(sort(dt_model$variable.importance, decreasing=TRUE))[1:2]
feat1<-top2[1]
feat2<-top2[2]

splits<-dt_model$splits
cut1<- splits[rownames(splits)==feat1, "index"][1]
cut2<-splits[rownames(splits)==feat2, "index"][1]

xrng<-range(test_set[[feat1]],na.rm=TRUE)
yrng<-range(test_set[[feat2]], na.rm=TRUE)

ggplot(test_set, aes_string(x=feat1, y=feat2, color="factor(quality.code)")) +
  geom_point(alpha=0.6, size=2) +
  geom_vline(xintercept=cut1, color= "darkgreen", size = 1) +
  geom_segment(x=xrng[1],xend =cut1,y=cut2,yend=cut2,color="darkgreen",size= 1)+
  annotate("text",x=mean(c(cut1, xrng[2])),y=yrng[1],label ="R1",
           vjust=-1, size=6, fontface="bold")+
  annotate("text",x=mean(c(xrng[1], cut1)),y=yrng[1],label ="R2",
           vjust=-1, size=6, fontface="bold") +
  annotate("text",x=mean(c(xrng[1], cut1)),y=yrng[2],label ="R3",
           vjust=1, size=6, fontface="bold")+
  scale_color_manual(values = c("0" = "#1f78b4", "1" = "#e31a1c")) +
  labs(title="Decision Regions",
    subtitle=sprintf("%s < %.3f    %s < %.3f", feat1, cut1, feat2, cut2),
    x=feat1, y=feat2, color="Quality")+
  theme_minimal()+
  theme(plot.title=element_text(size=16, face="bold", hjust=0.5),
    plot.subtitle=element_text(size=11, color="gray40", hjust=0.5))





#5.2 Decision Tree via Generalized Gini index
#Define the loss matrix
loss_mat<- matrix(
  c(0, 2,    
    1, 0),     
  nrow=2, byrow = TRUE,
  dimnames=list(pred= c("0","1"),
                true=c("0","1"))
)


#5.2 Decision Tree via Generalized Gini index
#Define the loss matrix
loss_mat<- matrix(
  c(0, 1.7,    
    1, 0),     
  nrow=2, byrow = TRUE,
  dimnames=list(pred= c("0","1"),
                true=c("0","1"))
)

# 2. Build the original decision tree via Generalized Gini index
dt_gen <-rpart(quality.code ~ ., data=train_set,method="class",
               parms=list(split="gini",loss=loss_mat),
               control= rpart.control(maxdepth=5, minsplit=20, minbucket=10, cp=0.001, maxsurrogate= 0))

best_cp_gen<-dt_gen$cptable[ which.min(dt_gen$cptable[,"xerror"]), "CP" ]
dt_pruned_gen<-prune(dt_gen, cp = best_cp_gen)
pred_class<-predict(dt_pruned_gen, test_set, type="class")
pred_prob<-predict(dt_pruned_gen,test_set, type="prob")[, "1"]

cm_dt_gen<-table(Pred=pred_class, True=test_set$quality.code)
print(cm_dt_gen)

tp_dt_gen <-cm_dt_gen["1","1"]
tn_dt_gen<-cm_dt_gen["0","0"]
fp_dt_gen<-cm_dt_gen["1","0"]
fn_dt_gen<-cm_dt_gen["0","1"]
metrics_dt_gen <- c(
  Accuracy=(tp_dt_gen+tn_dt_gen)/sum(cm_dt_gen),
  Precision=tp_dt_gen/(tp_dt_gen+fp_dt_gen),
  Recall=tp_dt_gen/(tp_dt_gen+fn_dt_gen),
  Specificity=tn_dt_gen/(tn_dt_gen+fp_dt_gen),
  NPV = tn_dt_gen / (tn_dt_gen + fn_dt_gen),
  F1=2*tp_dt_gen/(2*tp_dt_gen+fp_dt_gen+fn_dt_gen))
print(round(metrics_dt_gen, 4))

#Scatter plot
top2_gen<-names(sort(dt_gen$variable.importance, decreasing=TRUE))[1:2]
print(sort(dt_gen$variable.importance, decreasing=TRUE))
feat1_gen<-top2_gen[1]
feat2_gen<-top2_gen[2]

splits_gen<-dt_gen$splits
cut1_gen<- splits_gen[rownames(splits_gen)==feat1_gen, "index"][1]
cut2_gen<-splits_gen[rownames(splits_gen)==feat2_gen, "index"][1]

xrng_gen<-range(test_set[[feat1_gen]],na.rm=TRUE)
yrng_gen<-range(test_set[[feat2_gen]], na.rm=TRUE)

ggplot(test_set, aes_string(x =feat1_gen, y=feat2_gen, color="quality.code"))+
  geom_point(alpha=0.6, size=2)+
  geom_vline(xintercept =cut1_gen, color="darkgreen", size=1)+
  geom_segment(x =xrng_gen[1],xend=cut1_gen,y=cut2_gen, yend=cut2_gen,
               color="darkgreen", size=1) +
  annotate("text",x=mean(c(cut1_gen, xrng_gen[2])), y= yrng_gen[1],
           label="R1", vjust =-1, size=6, fontface="bold")+
  annotate("text", x=mean(c(xrng_gen[1],cut1_gen)), y=yrng_gen[1],
           label="R2", vjust=-1, size=6, fontface="bold")+
  annotate("text", x=mean(c(xrng_gen[1],cut1_gen)), y=yrng_gen[2],
           label="R3", vjust=1, size=6, fontface ="bold") +
  labs(title="Decision Regions via Generalized Gini Index",
       subtitle = sprintf("%s < %.3f    %s < %.3f",feat1_gen, cut1_gen, feat2_gen, cut2_gen),
       x=feat1_gen, y=feat2_gen, color="Quality") +
  theme_minimal()+
  theme(plot.title= element_text(size=16, face="bold", hjust=0.5),
        plot.subtitle=element_text(size=11, color="gray40", hjust=0.5)
  )
